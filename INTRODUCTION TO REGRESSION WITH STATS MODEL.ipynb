{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef8e17c-bd3c-4570-93da-51a6b56b3b32",
   "metadata": {},
   "source": [
    "# A TALE OF TWO VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a232d0-2fbd-4979-80a6-051196183478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Correlation between two columns directly\n",
    "corr_value = df[\"age\"].corr(df[\"salary\"])\n",
    "print(corr_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e803965-da2a-48bc-bce8-0288efc00fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before running reg models its good idea to viz your dataset\n",
    "import matplotlib \n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f6724-1cea-4042-aa78-982313705bb9",
   "metadata": {},
   "source": [
    "# Visualizing two numeric variables\n",
    "Before you can run any statistical models, it's usually a good idea to visualize your dataset. Here, you'll look at the relationship between house price per area and the number of nearby convenience stores using the Taiwan real estate dataset.\n",
    "\n",
    "One challenge in this dataset is that the number of convenience stores contains integer data, causing points to overlap. To solve this, you will make the points transparent.\n",
    "\n",
    "taiwan_real_estate is available as a pandas DataFrame.\n",
    "\n",
    "instruction\n",
    "\n",
    "Import the seaborn package, aliased as sns.\n",
    "\n",
    "Using taiwan_real_estate, draw a scatter plot of \"price_twd_msq\" (y-axis) versus \"n_convenience\" (x-axis).\n",
    "\n",
    "Draw a trend line calculated using linear regression. Omit the confidence interval ribbon. Note: The scatter_kws argument, pre-filled in the exercise, makes the data points 50% transparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e654f2-ce55-4e31-9b6b-88966c1d97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn with alias sns\n",
    "import seaborn as sns\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Draw the scatter plot\n",
    "sns.scatterplot(x=\"n_convenience\",\n",
    "                y=\"price_twd_msq\",\n",
    "                data=taiwan_real_estate)\n",
    "\n",
    "# Draw a trend line on the scatter plot of price_twd_msq vs. n_convenience\n",
    "sns.regplot(x=\"n_convenience\",\n",
    "        y=\"price_twd_msq\",\n",
    "         data=taiwan_real_estate,\n",
    "         ci=None,\n",
    "         scatter_kws={'alpha': 0.5})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa082e8e-4a44-4a2b-9509-608882bfaea9",
   "metadata": {},
   "source": [
    "# Linear regression with ols()\n",
    "While sns.regplot() can display a linear regression trend line, it doesn't give you access to the intercept and slope as variables, or allow you to work with the model results as variables. That means that sometimes you'll need to run a linear regression yourself.\n",
    "\n",
    "Time to run your first model!\n",
    "\n",
    "taiwan_real_estate is available. TWD is an abbreviation for Taiwan dollars.\n",
    "\n",
    "In addition, for this exercise and the remainder of the course, the following packages will be imported and aliased if necessary: matplotlib.pyplot as plt, seaborn as sns, and pandas as pd.\n",
    "\n",
    "Instructions 1/3\n",
    "\n",
    "Import the ols() function from the statsmodels.formula.api package.\n",
    "\n",
    "Run a linear regression with price_twd_msq as the response variable, n_convenience as the explanatory variable, and taiwan_real_estate as the dataset.\n",
    "\n",
    "Name it mdl_price_vs_conv.\n",
    "\n",
    "Fit the model.\n",
    "\n",
    "Print the parameters of the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc4d9d-dc7d-49c9-97da-b9010530166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ols function\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create the model object\n",
    "mdl_price_vs_conv = ols(\"price_twd_msq~n_convenience\", data= taiwan_real_estate) \n",
    "\n",
    "# Fit the model\n",
    "mdl_price_vs_conv = mdl_price_vs_conv.fit()\n",
    "\n",
    "# Print the parameters of the fitted model\n",
    "print(mdl_price_vs_conv.params)\n",
    "\n",
    "# Intercept  8.224 On average, a house with zero convenience stores nearby had a price of 8.2242 TWD per square meter.\n",
    "\n",
    "# n_convenience    0.798 If you increase the number of nearby convenience stores by one, \n",
    "# then the expected increase in house price is 0.7981 TWD per square meter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045379f7-a40c-4916-95ce-52aa84b28e25",
   "metadata": {},
   "source": [
    "# Visualizing numeric vs. categorical\n",
    "If the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn't make sense. Instead, a good option is to draw a histogram for each category.\n",
    "\n",
    "The Taiwan real estate dataset has a categorical variable in the form of the age of each house. The ages have been split into 3 groups: 0 to 15 years, 15 to 30 years, and 30 to 45 years.\n",
    "\n",
    "taiwan_real_estate is available.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Using taiwan_real_estate, plot a histogram of price_twd_msq with 10 bins. Split the plot by house_age_years to give 3 panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ba489-8b1a-4bd4-98b5-70262f0b6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of price_twd_msq with 10 bins, split by the age of each house\n",
    "sns.displot(data=taiwan_real_estate,\n",
    "         x=\"price_twd_msq\",\n",
    "         col = \"house_age_years\",\n",
    "         bins = 10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465975a-fcd1-4800-bef7-549bb5cb6a87",
   "metadata": {},
   "source": [
    "# Calculating means by category\n",
    "A good way to explore categorical variables further is to calculate summary statistics for each category. For example, you can calculate the mean and median of your response variable, grouped by a categorical variable. As such, you can compare each category in more detail.\n",
    "\n",
    "Here, you'll look at grouped means for the house prices in the Taiwan real estate dataset. This will help you understand the output of a linear regression with a categorical variable.\n",
    "\n",
    "taiwan_real_estate is available as a pandas DataFrame.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Group taiwan_real_estate by house_age_years and calculate the mean price (price_twd_msq) for each age group. Assign the result to mean_price_by_age.\n",
    "\n",
    "Print the result and inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5501ef7-8537-4160-acf5-b792f9ea410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of price_twd_msq, grouped by house age\n",
    "mean_price_by_age = taiwan_real_estate.groupby(\"house_age_years\")[\"price_twd_msq\"].mean()\n",
    "\n",
    "# Print the result\n",
    "print(mean_price_by_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1cc61-8b46-4dbf-96d9-4126dcc67e54",
   "metadata": {},
   "source": [
    "# Linear regression with a categorical explanatory variable\n",
    "Great job calculating those grouped means! As mentioned in the last video, the means of each category will also be the coefficients of a linear regression model with one categorical variable. You'll prove that in this exercise.\n",
    "\n",
    "To run a linear regression model with categorical explanatory variables, you can use the same code as with numeric explanatory variables. The coefficients returned by the model are different, however. Here you'll run a linear regression on the Taiwan real estate dataset.\n",
    "\n",
    "taiwan_real_estate is available and the ols() function is also loaded.\n",
    "\n",
    "Instructions \n",
    "\n",
    "Run and fit a linear regression with price_twd_msq as the response variable, house_age_years as the explanatory variable, and taiwan_real_estate as the dataset. Assign to mdl_price_vs_age.\n",
    "\n",
    "Print its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ddb2c-fa40-47f5-b592-ca6c5bd24143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model, fit it\n",
    "mdl_price_vs_age = ols(\"price_twd_msq~house_age_years\", data=taiwan_real_estate).fit()\n",
    "\n",
    "# Print the parameters of the fitted model\n",
    "print(mdl_price_vs_age.params)\n",
    "\n",
    "\n",
    "# Update the model formula to remove the intercept\n",
    "mdl_price_vs_age0 = ols(\"price_twd_msq ~ house_age_years + 0\", data=taiwan_real_estate).fit()\n",
    "\n",
    "# Print the parameters of the fitted model\n",
    "print(mdl_price_vs_age0.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19c4bd-8d3c-4130-8dba-e8de67087605",
   "metadata": {},
   "source": [
    "# Predicting house prices\n",
    "\n",
    "Perhaps the most useful feature of statistical models like linear regression is that you can make predictions. That is, you specify values for each of the explanatory variables, feed them to the model, and get a prediction for the corresponding response variable. The code flow is as follows.\n",
    "\n",
    "explanatory_data = pd.DataFrame({\"explanatory_var\": list_of_values})\n",
    "\n",
    "predictions = model.predict(explanatory_data)\n",
    "\n",
    "prediction_data = explanatory_data.assign(response_var=predictions)\n",
    "\n",
    "Here, you'll make predictions for the house prices in the Taiwan real estate dataset.\n",
    "\n",
    "taiwan_real_estate is available. The fitted linear regression model of house price versus number of convenience stores is available as mdl_price_vs_conv. For future exercises, when a model is available, it will also be fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823283b-6d35-49bd-b38a-2df521b48249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of explanatory data, where the number of convenience stores, n_convenience, takes the integer values from zero to ten.\n",
    "\n",
    "# Import numpy with alias np\n",
    "import numpy as np\n",
    "\n",
    "# Create the explanatory_data \n",
    "explanatory_data = pd.DataFrame({'n_convenience': np.arange(0,11)})\n",
    "\n",
    "# Print it\n",
    "print(explanatory_data)\n",
    "\n",
    "# Use the model mdl_price_vs_conv to make predictions from explanatory_data and store it as price_twd_msq.\n",
    "price_twd_msq = mdl_price_vs_conv.predict(explanatory_data)\n",
    "\n",
    "# Print it\n",
    "print(price_twd_msq)\n",
    "\n",
    "\n",
    "# Create a DataFrame of predictions named prediction_data. \n",
    "# Start with explanatory_data, then add an extra column, price_twd_msq, containing the predictions you created in the previous step.\n",
    "\n",
    "prediction_data = explanatory_data.assign(price_twd_msq = mdl_price_vs_conv.predict(explanatory_data))\n",
    "# Print the result\n",
    "print(prediction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c3b58-cd7a-4bcf-b857-98a92d8f309f",
   "metadata": {},
   "source": [
    "# Visualizing predictions\n",
    "\n",
    "The prediction DataFrame you created contains a column of explanatory variable values and a column of response variable values. That means you can plot it on the same scatter plot of response versus explanatory data values.\n",
    "\n",
    "prediction_data is available. The code for the plot you created using sns.regplot() in Chapter 1 is shown.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Create a new figure to plot multiple layers.\n",
    "\n",
    "Extend the plotting code to add points for the predictions in prediction_data. Color the points red.\n",
    "\n",
    "Display the layered plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227a54e-37f5-4d45-8197-41cd288573da",
   "metadata": {},
   "source": [
    "# Visualizing predictions\n",
    "The prediction DataFrame you created contains a column of explanatory variable values and a column of response variable values. That means you can plot it on the same scatter plot of response versus explanatory data values.\n",
    "\n",
    "prediction_data is available. The code for the plot you created using sns.regplot() in Chapter 1 is shown.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Create a new figure to plot multiple layers.\n",
    "    \n",
    "Extend the plotting code to add points for the predictions in prediction_data. Color the points red.\n",
    "    \n",
    "Display the layered plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bac63-cb40-401b-ad63-da4af8cbdb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure, fig\n",
    "fig = plt.figure()\n",
    "\n",
    "sns.regplot(x=\"n_convenience\",\n",
    "            y=\"price_twd_msq\",\n",
    "            data=taiwan_real_estate,\n",
    "            ci=None)\n",
    "# Add a scatter plot layer to the regplot\n",
    "sns.scatterplot(x=\"n_convenience\",\n",
    "            y=\"price_twd_msq\",\n",
    "            data=prediction_data,\n",
    "            color= \"red\", marker= \"s\")\n",
    "\n",
    "# Show the layered plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b60350-4356-45ae-ba68-4e466b358b99",
   "metadata": {},
   "source": [
    "# The limits of prediction\n",
    "In the last exercise, you made predictions on some sensible, could-happen-in-real-life, situations. That is, the cases when the number of nearby convenience stores were between zero and ten. To test the limits of the model's ability to predict, try some impossible situations.\n",
    "\n",
    "Use the console to try predicting house prices from mdl_price_vs_conv when there are -1 convenience stores. Do the same for 2.5 convenience stores. What happens in each case?\n",
    "\n",
    "mdl_price_vs_conv is available.\n",
    "\n",
    "Instructions \n",
    "\n",
    "Create some impossible explanatory data. Define a DataFrame impossible with one column, n_convenience, set to -1 in the first row, and 2.5 in the second row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c4113-4f40-4195-b969-fd124482334e",
   "metadata": {},
   "source": [
    "# Linear models don't know what is possible or not in real life. That means that they can give you predictions that don't make any sense when applied to your data. You need to understand what your data means in order to determine whether a prediction is nonsense or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436de7cb-b88c-4d2a-8d4b-236d0c65b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of defining a DataFrame with a dictionary\n",
    "import pandas as pd\n",
    "\n",
    "# Define a DataFrame impossible\n",
    "impossible = pd.DataFrame({\"n_convenience\":  [-1,2.5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b078e-73c9-458c-8847-3b24d61aaffc",
   "metadata": {},
   "source": [
    "# Extracting model elements\n",
    "The model object created by ols() contains many elements. In order to perform further analysis on the model results, you need to extract its useful bits. The model coefficients, the fitted values, and the residuals are perhaps the most important pieces of the linear model object.\n",
    "\n",
    "mdl_price_vs_conv is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdb8c3-a01e-414d-bc0a-ecf75d06c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters of mdl_price_vs_conv.\n",
    "print(mdl_price_vs_conv.params)\n",
    "\n",
    "# Print the fitted values of mdl_price_vs_conv.\n",
    "print(mdl_price_vs_conv.fittedvalues)\n",
    "    \n",
    "# Print the residuals of mdl_price_vs_conv.\n",
    "print(mdl_price_vs_conv.resid)\n",
    "\n",
    "# Print a summary of mdl_price_vs_conv.\n",
    "print(mdl_price_vs_conv.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9931c-ec0b-440c-8204-62cb09d1cf95",
   "metadata": {},
   "source": [
    "# Manually predicting house prices\n",
    "You can manually calculate the predictions from the model coefficients. When making predictions in real life, it is better to use .predict(), but doing this manually is helpful to reassure yourself that predictions aren't magic - they are simply arithmetic.\n",
    "\n",
    "In fact, for a simple linear regression, the predicted value is just the intercept plus the slope times the explanatory variable.\n",
    "\n",
    "\n",
    "mdl_price_vs_conv and explanatory_data are available.\n",
    "\n",
    "Get the coefficients/parameters of mdl_price_vs_conv, assigning to coeffs.\n",
    "    \n",
    "Get the intercept, which is the first element of coeffs, assigning to intercept.\n",
    "    \n",
    "Get the slope, which is the second element of coeffs, assigning to slope.\n",
    "    \n",
    "Manually predict price_twd_msq using the formula, specifying the intercept, slope, and explanatory_data.\n",
    "\n",
    "Run the code to compare your manually calculated predictions to the results from .predict()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf50c88-1968-437d-a19f-f096a6d1cf74",
   "metadata": {},
   "source": [
    "# Plotting consecutive portfolio returns\n",
    "Regression to the mean is also an important concept in investing. Here you'll look at the annual returns from investing in companies in the Standard and Poor 500 index (S&P 500), in 2018 and 2019.\n",
    "\n",
    "The sp500_yearly_returns dataset contains three columns:\n",
    "\n",
    "variable   meaning\n",
    "\n",
    "symbol     Stock ticker symbol uniquely identifying the company.\n",
    "\n",
    "return_2018\t A measure of investment performance in 2018.\n",
    "\n",
    "return_2019\tA measure of investment performance in 2019.\n",
    "\n",
    "A positive number for the return means the investment increased in value; negative means it lost value.\n",
    "\n",
    "Just as with baseball home runs, a naive prediction might be that the investment performance stays the same from year to year, lying on the y equals x line.\n",
    "\n",
    "sp500_yearly_returns is available as a pandas DataFrame.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Create a new figure, fig, to enable plot layering.\n",
    "\n",
    "Generate a line at y equals x. This has been done for you.\n",
    "\n",
    "Using sp500_yearly_returns, draw a scatter plot of return_2019 vs. return_2018 with a linear regression trend line, without a standard error ribbon.\n",
    "\n",
    "Set the axes so that the distances along the x and y axes look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14f3c2-b799-4803-9bec-c2791d4c18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure, fig\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot the first layer: y = x\n",
    "plt.axline(xy1=(0,0), slope=1, linewidth=2, color=\"green\")\n",
    "\n",
    "# Add scatter plot with linear regression trend line\n",
    "sns.regplot(x=\"return_2018\", y=\"return_2019\", data=sp500_yearly_returns, ci= None)\n",
    "\n",
    "# Set the axes so that the distances along the x and y axes look the same\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37887d1-ea71-41fc-9ae6-a85504c1ec1e",
   "metadata": {},
   "source": [
    "# Modeling consecutive returns\n",
    "Let's quantify the relationship between returns in 2019 and 2018 by running a linear regression and making predictions. By looking at companies with extremely high or extremely low returns in 2018, we can see if their performance was similar in 2019.\n",
    "\n",
    "sp500_yearly_returns is available and ols() is loaded.\n",
    "\n",
    "Instructions\n",
    "\n",
    "Create a DataFrame named explanatory_data. Give it one column (return_2018) with 2018 returns set to a list containing -1, 0, and 1.\n",
    "\n",
    "Use mdl_returns to predict with explanatory_data in a print() call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd26894-5dc6-499a-9c83-7e50ae265b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_returns = ols(\"return_2019 ~ return_2018\", data=sp500_yearly_returns).fit()\n",
    "\n",
    "# Create a DataFrame with return_2018 at -1, 0, and 1 \n",
    "explanatory_data = pd.DataFrame({\"return_2018\":[-1,0,1]})\n",
    "\n",
    "# Use mdl_returns to predict with explanatory_data\n",
    "print(mdl_returns.predict(explanatory_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad812b3-639e-45b6-b2ab-ec48e1ea5bf5",
   "metadata": {},
   "source": [
    "# Transforming the explanatory variable\n",
    "If there is no straight-line relationship between the response variable and the explanatory variable, it is sometimes possible to create one by transforming one or both of the variables. Here, you'll look at transforming the explanatory variable.\n",
    "\n",
    "You'll take another look at the Taiwan real estate dataset, this time using the distance to the nearest MRT (metro) station as the explanatory variable. You'll use code to make every commuter's dream come true: shortening the distance to the metro station by taking the square root. Take that, geography!\n",
    "\n",
    "taiwan_real_estate is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b978c-0003-4332-bdc5-64e065238eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sqrt_dist_to_mrt_m\n",
    "taiwan_real_estate[\"sqrt_dist_to_mrt_m\"] = np.sqrt(taiwan_real_estate[\"dist_to_mrt_m\"])\n",
    "\n",
    "# Run a linear regression of price_twd_msq vs. sqrt_dist_to_mrt_m\n",
    "mdl_price_vs_dist = ols(\"price_twd_msq ~ sqrt_dist_to_mrt_m\", data=taiwan_real_estate).fit()\n",
    "\n",
    "# Use this explanatory data\n",
    "explanatory_data = pd.DataFrame({\"sqrt_dist_to_mrt_m\": np.sqrt(np.arange(0, 81, 10) ** 2),\n",
    "                                \"dist_to_mrt_m\": np.arange(0, 81, 10) ** 2})\n",
    "\n",
    "# Use mdl_price_vs_dist to predict explanatory_data\n",
    "prediction_data = explanatory_data.assign(\n",
    "    price_twd_msq = mdl_price_vs_dist.predict(explanatory_data)\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.regplot(x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=taiwan_real_estate, ci=None)\n",
    "\n",
    "# Add a layer of your prediction points\n",
    "sns.scatterplot( x=\"sqrt_dist_to_mrt_m\", y=\"price_twd_msq\", data=prediction_data, color=\"red\", marker=\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1818fd-88f5-40ba-af5f-9c4b5ed37f24",
   "metadata": {},
   "source": [
    "# Transforming the response variable too\n",
    "The response variable can be transformed too, but this means you need an extra step at the end to undo that transformation. That is, you \"back transform\" the predictions.\n",
    "\n",
    "In the video, you saw the first step of the digital advertising workflow: spending money to buy ads, and counting how many people see them (the \"impressions\"). The next step is determining how many people click on the advert after seeing it.\n",
    "\n",
    "ad_conversion is available as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef96237-e4af-4bc5-8211-719452369df1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ad_conversion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ad_conversion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqdrt_n_impressions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ad_conversion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_impressions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m      2\u001b[0m ad_conversion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqdrt_n_clicks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ad_conversion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_clicks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m      4\u001b[0m mdl_click_vs_impression \u001b[38;5;241m=\u001b[39m ols(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqdrt_n_clicks ~ qdrt_n_impressions\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mad_conversion, ci\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfit()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ad_conversion' is not defined"
     ]
    }
   ],
   "source": [
    "ad_conversion[\"qdrt_n_impressions\"] = ad_conversion[\"n_impressions\"] ** 0.25\n",
    "ad_conversion[\"qdrt_n_clicks\"] = ad_conversion[\"n_clicks\"] ** 0.25\n",
    "\n",
    "mdl_click_vs_impression = ols(\"qdrt_n_clicks ~ qdrt_n_impressions\", data=ad_conversion, ci=None).fit()\n",
    "\n",
    "explanatory_data = pd.DataFrame({\"qdrt_n_impressions\": np.arange(0, 3e6+1, 5e5) ** .25,\n",
    "                                 \"n_impressions\": np.arange(0, 3e6+1, 5e5)})\n",
    "# Since the response variable has been transformed, you'll now need to back-transform the predictions to correctly interpret your results.\n",
    "\n",
    "# Complete prediction_data\n",
    "prediction_data = explanatory_data.assign(\n",
    "    qdrt_n_clicks = mdl_click_vs_impression.predict(explanatory_data)\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(prediction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df9e23-0507-4a5b-a72a-df2a3ffc2808",
   "metadata": {},
   "source": [
    "# Back transformation\n",
    "In the previous exercise, you transformed the response variable, ran a regression, and made predictions. But you're not done yet! In order to correctly interpret and visualize your predictions, you'll need to do a back-transformation.\n",
    "\n",
    "prediction_data, which you created in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd5d20-7470-491c-a1cf-2bb8f904b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back transform qdrt_n_clicks\n",
    "prediction_data[\"n_clicks\"] = prediction_data[\"qdrt_n_clicks\"] ** 4\n",
    "\n",
    "# Plot the transformed variables\n",
    "fig = plt.figure()\n",
    "sns.regplot(x=\"qdrt_n_impressions\", y=\"qdrt_n_clicks\", data=ad_conversion, ci=None)\n",
    "\n",
    "# Add a layer of your prediction points\n",
    "sns.scatterplot( x=\"qdrt_n_impressions\", y=\"qdrt_n_clicks\", data=prediction_data, color=\"red\", marker=\"s\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b2b19-15ff-4e40-b608-27c888d994eb",
   "metadata": {},
   "source": [
    "# Coefficient of determination\n",
    "The coefficient of determination is a measure of how well the linear regression line fits the observed values. For simple linear regression, it is equal to the square of the correlation between the explanatory and response variables.\n",
    "\n",
    "Here, you'll take another look at the second stage of the advertising pipeline: modeling the click response to impressions. Two models are available: mdl_click_vs_impression_orig models n_clicks versus n_impressions. mdl_click_vs_impression_trans is the transformed model you saw in Chapter 2. It models n_clicks to the power of 0.25 versus n_impressions to the power of 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6004f-38bb-48b9-9a77-f9d77d64f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of mdl_click_vs_impression_orig\n",
    "print(mdl_click_vs_impression_orig.summary())\n",
    "\n",
    "# Print a summary of mdl_click_vs_impression_trans\n",
    "print(mdl_click_vs_impression_trans.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad97eeb-9f33-41ec-8b76-b59a059532c3",
   "metadata": {},
   "source": [
    "\n",
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:               n_clicks   R-squared:                       0.892\n",
    "    Model:                            OLS   Adj. R-squared:                  0.891\n",
    "    Method:                 Least Squares   F-statistic:                     7683.\n",
    "    Date:                Tue, 14 Oct 2025   Prob (F-statistic):               0.00\n",
    "    Time:                        03:41:01   Log-Likelihood:                -4126.7\n",
    "    No. Observations:                 936   AIC:                             8257.\n",
    "    Df Residuals:                     934   BIC:                             8267.\n",
    "    Df Model:                           1                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    =================================================================================\n",
    "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ---------------------------------------------------------------------------------\n",
    "    Intercept         1.6829      0.789      2.133      0.033       0.135       3.231\n",
    "    n_impressions     0.0002   1.96e-06     87.654      0.000       0.000       0.000\n",
    "    ==============================================================================\n",
    "    Omnibus:                      247.038   Durbin-Watson:                   0.870\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            13215.277\n",
    "    Skew:                          -0.258   Prob(JB):                         0.00\n",
    "    Kurtosis:                      21.401   Cond. No.                     4.88e+05\n",
    "    ==============================================================================\n",
    "    \n",
    "    Notes:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "    [2] The condition number is large, 4.88e+05. This might indicate that there are\n",
    "    strong multicollinearity or other numerical problems.\n",
    "\n",
    "\n",
    "\n",
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:          qdrt_n_clicks   R-squared:                       0.945\n",
    "    Model:                            OLS   Adj. R-squared:                  0.944\n",
    "    Method:                 Least Squares   F-statistic:                 1.590e+04\n",
    "    Date:                Tue, 14 Oct 2025   Prob (F-statistic):               0.00\n",
    "    Time:                        03:41:01   Log-Likelihood:                 193.90\n",
    "    No. Observations:                 936   AIC:                            -383.8\n",
    "    Df Residuals:                     934   BIC:                            -374.1\n",
    "    Df Model:                           1                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ======================================================================================\n",
    "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    --------------------------------------------------------------------------------------\n",
    "    Intercept              0.0717      0.017      4.171      0.000       0.038       0.106\n",
    "    qdrt_n_impressions     0.1115      0.001    126.108      0.000       0.110       0.113\n",
    "    ==============================================================================\n",
    "    Omnibus:                       11.447   Durbin-Watson:                   0.568\n",
    "    Prob(Omnibus):                  0.003   Jarque-Bera (JB):               10.637\n",
    "    Skew:                          -0.216   Prob(JB):                      0.00490\n",
    "    Kurtosis:                       2.707   Cond. No.                         52.1\n",
    "    ==============================================================================\n",
    "    \n",
    "    Notes:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specifie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc172010-77c7-4f97-af3e-649d5fe1d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coeff of determination for mdl_click_vs_impression_orig\n",
    "print(mdl_click_vs_impression_orig.rsquared)\n",
    "\n",
    "# Print the coeff of determination for mdl_click_vs_impression_trans\n",
    "print(mdl_click_vs_impression_trans.rsquared)\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#    0.8916134973508041\n",
    "#    0.9445272817143905\n",
    "\n",
    "# The number of impressions explains 89% of the variability in the number of clicks.\n",
    "\n",
    "# The transformed model, mdl_click_vs_impression_trans gives a better fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528db25e-1121-4d78-b8f7-7ca896db891e",
   "metadata": {},
   "source": [
    "# Residual standard error\n",
    "Residual standard error (RSE) is a measure of the typical size of the residuals. Equivalently, it's a measure of how wrong you can expect predictions to be. Smaller numbers are better, with zero being a perfect fit to the data.\n",
    "\n",
    "Again, you'll look at the models from the advertising pipeline, mdl_click_vs_impression_orig and mdl_click_vs_impression_trans.\n",
    "\n",
    "\n",
    "# RSE is a measure of accuracy for regression models. It even works on other other statistical model types like regression trees, so you can compare accuracy across different classes of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e3006f-5633-4dac-89f4-82b188869a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mdl_click_vs_impression_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate mse_orig for mdl_click_vs_impression_orig\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mse_orig \u001b[38;5;241m=\u001b[39m mdl_click_vs_impression_orig\u001b[38;5;241m.\u001b[39mmse_resid\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate rse_orig for mdl_click_vs_impression_orig and print it\u001b[39;00m\n\u001b[0;32m      5\u001b[0m rse_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse_orig)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mdl_click_vs_impression_orig' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate mse_orig for mdl_click_vs_impression_orig\n",
    "mse_orig = mdl_click_vs_impression_orig.mse_resid\n",
    "\n",
    "# Calculate rse_orig for mdl_click_vs_impression_orig and print it\n",
    "rse_orig = np.sqrt(mse_orig)\n",
    "print(\"RSE of original model: \", rse_orig)\n",
    "\n",
    "# Calculate mse_trans for mdl_click_vs_impression_trans\n",
    "mse_trans = mdl_click_vs_impression_trans.mse_resid\n",
    "\n",
    "# Calculate rse_trans for mdl_click_vs_impression_trans and print it\n",
    "rse_trans = np.sqrt(mse_trans)\n",
    "print(\"RSE of transformed model: \", rse_trans)\n",
    "\n",
    "# <script.py> output:\n",
    "#    RSE of original model:  19.905838862478138\n",
    "#    RSE of transformed model:  0.19690640896875722\n",
    "\n",
    "# The typical difference between observed number of clicks and predicted number of clicks is 20.\n",
    "\n",
    "# The transformed model, mdl_click_vs_impression_trans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ceae7c-f7c5-4fe7-a100-a43b5f296a66",
   "metadata": {},
   "source": [
    "# Drawing diagnostic plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a18a8-9318-4af3-9018-120d6e3ea8c7",
   "metadata": {},
   "source": [
    "It's time for you to draw these diagnostic plots yourself using the Taiwan real estate dataset and the model of house prices versus number of convenience stores.\n",
    "\n",
    "taiwan_real_estate is available as a pandas DataFrame and mdl_price_vs_conv is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f2548-0a49-4699-9c34-74b59efb0717",
   "metadata": {},
   "source": [
    "## residuals vs. fitted values (residplot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c7a7b-b29a-47d7-9305-8c46a1176d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the residuals versus fitted values plot. Add a lowess argument to visualize the trend of the residuals.\n",
    "\n",
    "# Plot the residuals vs. fitted values\n",
    "sns.residplot(x=\"n_convenience\", y=\"price_twd_msq\", data=taiwan_real_estate, lowess=True)\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f2580-0524-47d3-9ee7-009fbf852f59",
   "metadata": {},
   "source": [
    "## Create the Q-Q plot of the residuals. (Q-Q plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2661a01-daf8-47a2-ba71-6f47813f89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import qqplot\n",
    "from statsmodels.api import qqplot \n",
    "\n",
    "# Create the Q-Q plot of the residuals\n",
    "qqplot(data=mdl_price_vs_conv.resid, fit=True, line=\"45\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show() \n",
    "\n",
    "# residual ofthe model is the data arguement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39fe8dd-43fd-45b5-b8f8-04fff812f34c",
   "metadata": {},
   "source": [
    "## Create the scale-location plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a30a4-b1a1-4f9e-b536-261247feb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "model_norm_residuals = mdl_price_vs_conv.get_influence().resid_studentized_internal\n",
    "model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n",
    "\n",
    "# Create the scale-location plot\n",
    "sns.regplot(x=mdl_price_vs_conv.fittedvalues, y=model_norm_residuals_abs_sqrt, ci=None, lowess=True)\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Sqrt of abs val of stdized residuals\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936b707-6485-45f4-9f1c-12703f444b1e",
   "metadata": {},
   "source": [
    "# Leverage\n",
    "Leverage measures how unusual or extreme the explanatory variables are for each observation. Very roughly, high leverage means that the explanatory variable has values that are different from other points in the dataset. In the case of simple linear regression, where there is only one explanatory value, this typically means values with a very high or very low explanatory value.\n",
    "\n",
    "Here, you'll look at highly leveraged values in the model of house price versus the square root of distance from the nearest MRT station in the Taiwan real estate dataset.\n",
    "\n",
    "\n",
    "Answer\n",
    "\n",
    "Observations with a large distance to the nearest MRT station have the highest leverage, because most of the observations have a short distance, so long distances are more extreme.  Highly leveraged points are the ones with explanatory variables that are furthest away from the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e40256-8a88-41f1-b1af-253d6eeb6240",
   "metadata": {},
   "source": [
    "# Influence\n",
    "Influence measures how much a model would change if each observation was left out of the model calculations, one at a time. That is, it measures how different the prediction line would look if you would run a linear regression on all data points except that point, compared to running a linear regression on the whole dataset.\n",
    "\n",
    "The standard metric for influence is Cook's distance, which calculates influence based on the residual size and the leverage of the point.\n",
    "\n",
    "You can see the same model as last time: house price versus the square root of distance from the nearest MRT station in the Taiwan real estate dataset.\n",
    "\n",
    "Guess which observations you think will have a high influence, then move the slider to find out.\n",
    "\n",
    "Which statement is true?\n",
    "\n",
    "Answer\n",
    "\n",
    "Observations far away from the trend line have high influence, because they have large residuals and are far away from other observations. The majority of the influential houses were those with prices that were much higher than the model predicted (and one with a price that was much lower)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e5831-bd17-43ca-b1db-1f3ec51d625b",
   "metadata": {},
   "source": [
    "# Extracting leverage and influence\n",
    "In the last few exercises, you explored which observations had the highest leverage and influence. Now you'll extract those values from the model.\n",
    "\n",
    "mdl_price_vs_dist and taiwan_real_estate are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbdafb8-d493-4d90-84eb-41e685cea923",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mdl_price_vs_dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get the summary frame from mdl_price_vs_dist and save as summary_info\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m summary_info \u001b[38;5;241m=\u001b[39m mdl_price_vs_dist\u001b[38;5;241m.\u001b[39mget_influence()\u001b[38;5;241m.\u001b[39msummary_frame()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Add the hat_diag column to taiwan_real_estate, name it leverage\u001b[39;00m\n\u001b[0;32m      5\u001b[0m taiwan_real_estate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleverage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m summary_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhat_diag\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mdl_price_vs_dist' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the summary frame from mdl_price_vs_dist and save as summary_info\n",
    "summary_info = mdl_price_vs_dist.get_influence().summary_frame()\n",
    "\n",
    "# Add the hat_diag column to taiwan_real_estate, name it leverage\n",
    "taiwan_real_estate[\"leverage\"] = summary_info[\"hat_diag\"]\n",
    "\n",
    "# Sort taiwan_real_estate by leverage in descending order and print the head\n",
    "print(taiwan_real_estate.sort_values(\"leverage\", ascending=False).head())\n",
    "\n",
    "\n",
    "# Add the cooks_d column to taiwan_real_estate, name it cooks_dist\n",
    "taiwan_real_estate[\"cooks_dist\"]= summary_info[\"cooks_d\"]\n",
    "\n",
    "# Sort taiwan_real_estate by cooks_dist in descending order and print the head.\n",
    "print(taiwan_real_estate.sort_values(\"cooks_dist\", ascending= False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a630aa-4985-4573-a895-55a65bb83ee0",
   "metadata": {},
   "source": [
    "# Exploring the explanatory variables\n",
    "When the response variable is logical, all the points lie on the \n",
    " and \n",
    " lines, making it difficult to see what is happening. In the video, until you saw the trend line, it wasn't clear how the explanatory variable was distributed on each line. This can be solved with a histogram of the explanatory variable, grouped by the response.\n",
    "\n",
    "You will use these histograms to get to know the financial services churn dataset seen in the video.\n",
    "\n",
    "churn is available as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc65f5-10aa-4f18-b920-6687dbb8a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histograms of time_since_last_purchase split by has_churned\n",
    "# Make sure to specify the data and the column to split by\n",
    "sns.displot(data=churn,\n",
    "            x=\"time_since_last_purchase\",\n",
    "            col=\"has_churned\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Redraw the plot with time_since_first_purchase\n",
    "sns.displot(data= churn, x= \"time_since_first_purchase\", col=\"has_churned\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f9f5c-9395-4a58-9671-76b4705eb8a4",
   "metadata": {},
   "source": [
    "# Visualizing linear and logistic models\n",
    "As with linear regressions, regplot() will draw model predictions for a logistic regression without you having to worry about the modeling code yourself. To see how the predictions differ for linear and logistic regressions, try drawing both trend lines side by side. Spoiler: you should see a linear (straight line) trend from the linear model, and a logistic (S-shaped) trend from the logistic model.\n",
    "\n",
    "churn is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3c21f-86c5-4b40-a6d3-d3db0871b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\n",
    "sns.regplot(x=\"time_since_first_purchase\",\n",
    "            y=\"has_churned\",\n",
    "            data=churn, \n",
    "            ci=None,\n",
    "            line_kws={\"color\": \"red\"})\n",
    "\n",
    "# Draw a logistic regression trend line and a scatter plot of time_since_first_purchase vs. has_churned\n",
    "sns.regplot(x=\"time_since_first_purchase\",\n",
    "            y=\"has_churned\",\n",
    "            data=churn, \n",
    "            ci=None,\n",
    "            logistic=True,\n",
    "            line_kws={\"color\": \"blue\"})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a950db-e16f-4a3e-b77e-96ac36b9893f",
   "metadata": {},
   "source": [
    "# Logistic regression with logit()\n",
    "Logistic regression requires another function from statsmodels.formula.api: logit(). It takes the same arguments as ols(): a formula and data argument. You then use .fit() to fit the model to the data.\n",
    "\n",
    "Here, you'll model how the length of relationship with a customer affects churn.\n",
    "\n",
    "churn is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566112ca-5172-4498-b8ba-c3412f2b0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import logit\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "# Fit a logistic regression of churn vs. length of relationship using the churn dataset\n",
    "mdl_churn_vs_relationship = logit(\"has_churned~time_since_first_purchase\", data= churn).fit()\n",
    "\n",
    "# Print the parameters of the fitted model\n",
    "print(mdl_churn_vs_relationship.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ba986-3d79-40fe-96b3-7746535558d1",
   "metadata": {},
   "source": [
    "# Probabilities\n",
    "There are four main ways of expressing the prediction from a logistic regression model – we'll look at each of them over the next four exercises. Firstly, since the response variable is either \"yes\" or \"no\", you can make a prediction of the probability of a \"yes\". Here, you'll calculate and visualize these probabilities.\n",
    "\n",
    "Two variables are available:\n",
    "\n",
    "mdl_churn_vs_relationship is the fitted logistic regression model of has_churned versus time_since_first_purchase.\n",
    "explanatory_data is a DataFrame of explanatory values.\n",
    "\n",
    "Instructions 1/2\n",
    "\n",
    "Create a DataFrame, prediction_data, by assigning a column has_churned to explanatory_data.\n",
    "\n",
    "In the has_churned column, store the predictions of the probability of churning: use the model, mdl_churn_vs_relationship, and the explanatory data, explanatory_data.\n",
    "\n",
    "Print the first five lines of the prediction DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1f498-60bc-4752-8eb5-e5a452968947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction_data\n",
    "prediction_data = explanatory_data.assign(\n",
    "  has_churned=mdl_churn_vs_relationship.predict(explanatory_data)\n",
    "  \n",
    ")\n",
    "\n",
    "# Print the head\n",
    "print(prediction_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c3c54-ef88-499f-84d0-20aed1803f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4adcc-5ead-461f-8bf2-cc7b1828e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction_data\n",
    "prediction_data = explanatory_data.assign(\n",
    "    has_churned = mdl_churn_vs_relationship.predict(explanatory_data)\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create a scatter plot with logistic trend line\n",
    "sns.regplot(x=\"time_since_first_purchase\",\n",
    "            y=\"has_churned\",\n",
    "            data=churn,\n",
    "            ci=None,\n",
    "            logistic=True)\n",
    "\n",
    "# Overlay with prediction_data, colored red\n",
    "sns.scatterplot(x=\"time_since_first_purchase\",\n",
    "                y=\"has_churned\",\n",
    "                data=prediction_data,\n",
    "                color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5c3fc-043a-4fe0-9d60-3df5ae00234a",
   "metadata": {},
   "source": [
    "# Most likely outcome\n",
    "# Providing the most likely response is a great way to share the model results with a non-technical audience.\n",
    "When explaining your results to a non-technical audience, you may wish to side-step talking about probabilities and simply explain the most likely outcome. That is, rather than saying there is a 60% chance of a customer churning, you say that the most likely outcome is that the customer will churn. The trade-off here is easier interpretation at the cost of nuance.\n",
    "\n",
    "mdl_churn_vs_relationship, explanatory_data, and prediction_data are available from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e82801-b8c6-4aef-b8b3-abd45e721a5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Update prediction_data to add a column of the most likely churn outcome, most_likely_outcome. Print the first five lines of prediction_data.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m prediction_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost_likely_outcome\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(prediction_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_churned\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print the head\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(prediction_data\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Update prediction_data to add a column of the most likely churn outcome, most_likely_outcome. Print the first five lines of prediction_data.\n",
    "\n",
    "prediction_data[\"most_likely_outcome\"] = np.round(prediction_data[\"has_churned\"])\n",
    "# Print the head\n",
    "print(prediction_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798bd688-9348-401d-befb-ce96a416f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update prediction data by adding most_likely_outcome\n",
    "prediction_data[\"most_likely_outcome\"] = np.round(prediction_data[\"has_churned\"])\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create a scatter plot with logistic trend line (from previous exercise)\n",
    "sns.regplot(x=\"time_since_first_purchase\",\n",
    "            y=\"has_churned\",\n",
    "            data=churn,\n",
    "            ci=None,\n",
    "            logistic=True)\n",
    "\n",
    "# Overlay with prediction_data, colored red\n",
    "sns.scatterplot(x=\"time_since_first_purchase\",\n",
    "            y=\"most_likely_outcome\",\n",
    "        data= prediction_data, color= \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058bd482-5cdd-414f-9fee-8296381fd8cf",
   "metadata": {},
   "source": [
    "# Odds ratio\n",
    "Odds ratios compare the probability of something happening with the probability of it not happening. This is sometimes easier to reason about than probabilities, particularly when you want to make decisions about choices. For example, if a customer has a 20% chance of churning, it may be more intuitive to say \"the chance of them not churning is four times higher than the chance of them churning\".\n",
    "\n",
    "mdl_churn_vs_relationship, explanatory_data, and prediction_data are available from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe95660-d1c9-450e-a096-9222028216db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update prediction data with odds_ratio\n",
    "prediction_data[\"odds_ratio\"] = prediction_data[\"has_churned\"] / (1 - prediction_data[\"has_churned\"])\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create a line plot of odds_ratio vs time_since_first_purchase\n",
    "sns.lineplot(x=\"time_since_first_purchase\", y=\"odds_ratio\", data= prediction_data)\n",
    "\n",
    "# Add a dotted horizontal line at odds_ratio = 1\n",
    "plt.axhline(y=1, linestyle=\"dotted\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d9f06-59d5-4e2e-ac95-c760e47c7a45",
   "metadata": {},
   "source": [
    "# Log odds ratio\n",
    "One downside to probabilities and odds ratios for logistic regression predictions is that the prediction lines for each are curved. This makes it harder to reason about what happens to the prediction when you make a change to the explanatory variable. The logarithm of the odds ratio (the \"log odds ratio\" or \"logit\") does have a linear relationship between predicted response and explanatory variable. That means that as the explanatory variable changes, you don't see dramatic changes in the response metric - only linear changes.\n",
    "\n",
    "Since the actual values of log odds ratio are less intuitive than (linear) odds ratio, for visualization purposes it's usually better to plot the odds ratio and apply a log transformation to the y-axis scale.\n",
    "\n",
    "mdl_churn_vs_relationship, explanatory_data, and prediction_data are available from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c1bd4-f38c-4dc0-9fc1-b8333e72ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update prediction data with log_odds_ratio\n",
    "prediction_data[\"log_odds_ratio\"] = np.log(prediction_data[\"odds_ratio\"])\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Update the line plot: log_odds_ratio vs. time_since_first_purchase\n",
    "sns.lineplot(x=\"time_since_first_purchase\",\n",
    "             y=\"log_odds_ratio\",\n",
    "             data=prediction_data)\n",
    "\n",
    "# Add a dotted horizontal line at log_odds_ratio = 0\n",
    "plt.axhline(y=0, linestyle=\"dotted\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644627e-f1ee-457f-bf49-02b37ecd22b2",
   "metadata": {},
   "source": [
    "# Calculating the confusion matrix\n",
    "A confusion matrix (occasionally called a confusion table) is the basis of all performance metrics for models with a categorical response (such as a logistic regression). It contains the counts of each actual response-predicted response pair. In this case, where there are two possible responses (churn or not churn), there are four overall outcomes.\n",
    "\n",
    "True positive: The customer churned and the model predicted they would.\n",
    "False positive: The customer didn't churn, but the model predicted they would.\n",
    "True negative: The customer didn't churn and the model predicted they wouldn't.\n",
    "False negative: The customer churned, but the model predicted they wouldn't.\n",
    "churn and mdl_churn_vs_relationship are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3eb09-a439-49b5-a0c7-d0ef4c0edf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual responses by subsetting the has_churned column of the dataset. Assign to actual_response.\n",
    "actual_response = churn[\"has_churned\"]\n",
    "\n",
    "# Get the \"most likely\" predicted responses from the model. Assign to predicted_response.\n",
    "predicted_response = np.round(mdl_churn_vs_relationship.predict())\n",
    "\n",
    "# Create a DataFrame from actual_response and predicted_response. Assign to outcomes.\n",
    "outcomes = pd.DataFrame({\"actual_response\":actual_response,\n",
    "                         \"predicted_response\":predicted_response})\n",
    "\n",
    "# Print outcomes as a table of counts, representing the confusion matrix. This has been done for you.\n",
    "print(outcomes.value_counts(sort = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8bbe2c-c9c6-4a89-a73e-78c041904201",
   "metadata": {},
   "source": [
    "# Drawing a mosaic plot of the confusion matrix\n",
    "While calculating the performance matrix might be fun, it would become tedious if you needed multiple confusion matrices of different models. Luckily, the .pred_table() method can calculate the confusion matrix for you.\n",
    "\n",
    "Additionally, you can use the output from the .pred_table() method to visualize the confusion matrix, using the mosaic() function.\n",
    "\n",
    "churn and mdl_churn_vs_relationship are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d6293-d419-4415-9dd8-696e7d05c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mosaic from statsmodels.graphics.mosaicplot\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "# Create conf_matrix using the .pred_table() method and print it. Calculate the confusion matrix conf_matrix\n",
    "conf_matrix = mdl_churn_vs_relationship.pred_table()\n",
    "\n",
    "# Print it\n",
    "print(conf_matrix)\n",
    "\n",
    "# Draw a mosaic plot of conf_matrix\n",
    "mosaic(conf_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bae6b5-1a46-4211-829c-b391ff5e8cbe",
   "metadata": {},
   "source": [
    "# Accuracy, sensitivity, specificity\n",
    "Lots of performance metrics can be computed from a confusion matrix. For logistic regression, three of them in particular are important: accuracy, sensitivity, and specificity. Can you identify what each of those terms mean in the context of the churn model?\n",
    "\n",
    "# Measuring logistic model performance\n",
    "As you know by now, several metrics exist for measuring the performance of a logistic regression model. In this last exercise, you'll manually calculate accuracy, sensitivity, and specificity. Recall the following definitions:\n",
    "\n",
    "Accuracy is the proportion of predictions that are correct.\n",
    " \n",
    "\n",
    "Sensitivity is the proportion of true observations that are correctly predicted by the model as being true.\n",
    " \n",
    "\n",
    "Specificity is the proportion of false observations that are correctly predicted by the model as being false.\n",
    " \n",
    "\n",
    "churn, mdl_churn_vs_relationship, and conf_matrix are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce318b0-4802-4119-847d-f4b398b547cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TN, TP, FN and FP from conf_matrix\n",
    "TN = conf_matrix[0,0]\n",
    "TP = conf_matrix[1,1]\n",
    "FN = conf_matrix[1,0]\n",
    "FP = conf_matrix[0,1]\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = (TN+TP)/(TN+TP+FN+FP)\n",
    "print(\"accuracy: \", accuracy)\n",
    "\n",
    "# Calculate and print the sensitivity\n",
    "sensitivity = TP/(FN+TP)\n",
    "print(\"sensitivity: \", sensitivity)\n",
    "\n",
    "# Calculate and print the specificity\n",
    "specificity = TN/(TN+FP)\n",
    "print(\"specificity: \", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67337b4-952f-4b39-a8fb-5e16d233f57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8ce10-5d7f-451a-85d1-ffbe6e1a1be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8bb66-9008-4a89-8e9f-6ac3f82a158b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65218fa-d77b-48c2-93d6-fb888392e556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
